LOGIN: OPP
PASSPHRASE: o19p82pz

КЛАСТЕР:
    login: hpcuser231
    password: 2022*FIT (1608Denis@)
nusc.nsu.ru

select=2 - число чанков а не узлов
чанки можно размещать на одном или нескольких узлах
place=scatter - на разных узлах

Лабораторная работа №1 -- 3 Вариант!

Распараллеливание вычислений
Процессорные ядра выполняют работу

Квант - время непрерывной работы 
Hyperthreading - выделение виртуальных ядер

Физическими исполнителями являются процессорные ядра а также сами машины

Ускорить исполнение программы можно путём объединения нескольких
ядер или машин

Одновременно работающие программные сущности, которые можно породить - ПРОЦЕССЫ и ПОТОКИ

!!!Поток - часть процесса!!!

В каждом процессе существует хотя бы один поток 
Процесс - контейнер потоков
Поток - активно работающая сущность
Ограничения на количество потоков в одном процессе нет

Можно породить несколько процессов, а можно породить в пределах одного процесса несколько потоков
!!!Принципиальная разница: У каждого процесса своё адресное пространство(своя область памяти, недоступная другому проессу)
Если в каждом процессе объявили переменную, называющуюся одинаково, то это будут разные переменные,
не имеющие ничего общего между собой

!!!Потоки между собой делят адресное пространство процесса!!!
При изменении одним потоком общей переменной (shared), все остальные потоки увидят это изменение
Но в потоках также могут быть внуренние переменные (private)

Программировать в терминах потоков удобно
Технология параллельного программирования для процессов - MPI, для потоков - Pthreads, OpenMP
Mutex - конкреный элемент технологии, использующий Pthreads и OpenMP

Как налаживать общение между процессами?
send(переменная) ---> recieve(переменная)

программировать в MPI сложнее, в OpenMp проще

Процессы можно рассадить по разным узлам(вычислительным машинам - серверам)
Общение между серверами осуществляетс с помощью сети

Рассадить потоки процесса по разным машинам нельзя, т.к. они привязаны к ОС 
Процесс существует  только в пределах одной ОС

OpenMP годится, когда достаточно мощности одной машины


5 лабораторных работ с сайта ssd.sscc.ru
Работ меньше -> они объемнее
Работы более равномерные по сложности между собой,
направлены на составление параллельного алгоритма (на подумать)

Общий балл за курс складывается из оценок за лабы + контрольная работа(выполняется на паре)
Максим Александрович отвечает за итогвый зачет

Лабораторная работа №1
Для чего параллелизм?
Использовать MPI
Черпать информацию из лекций Максима Александровича и его презентаций!

Итерационные методы

x - 1 - cosx = 0
Что можно сказать о решении данного уравнения?
Существовует и единственное!

y1 = cosx
y2 = x - 1
Одна точка пересечения, находится между 1 и pi/2
Надо получить решение с точностью до пятого знака

Один из вариантов итерационного метода: разложить в ряд Тейлора
и поситать первые 1000 членов, сложить их, получим приближение
Точного ответа у таких уравнений(трансцендентных) нет

Без использования ряда Тейлора:
x = 1 + cosx
x(n+1) = 1 + cosx(n) --- Итерационный алгоритм 
В случае ряда Тейлора ряд сойдется к нужному ответу
В нашем случае ряд может не сойтись!!!

lim(x(n)) = a ?
Пусть сходится
Надо проверить, является ли полученное а решением исходного уравнения
lim(x(n+1)) = lim(1 + cosx(n))
a = lim(1 + cosx(n)) --> сходится последовательность x(n)
a = 1 + cosa

Еще вопросы перед составлением программы:
1)Начальные значения, т.к. формула рекурсивная (x(0) = 1/2*(1 + pi/2))
2)Определить количество итераций (когда мы должны остановиться?)
(модуль разности x(n+1) и x(n) < Эпсилон = 10^(-5)) - точность ответа

Приближения(x0, x1, x2, x3,...) могут прыгать достаточно хаотично, но при стремлении n к бесконечности, они будут сходится к а 
Если один раз оказались в эпсилон-интервале - совсем не гарантия, 
что дальше будем находиться в этом интервале (зависит от сложности выражения)
Для гарантии точности ответа рекомендуется такую оценку делать несколько раз (хотя бы 3 или 5)

|x(n) - x(n+1)| < E
|x(n-1) - x(n-2)| < E
|x(n-2) - x(n-3)| < E

Параллельное программирование нужно людям, котоыре имеют большую задачу и ограниченную мощность
Основная задача - ускорить вычисления! (прикладной программист использует любые средства для ускорения)
Думать как можно ускорить!

Умножение матрицы - 3 цикла
При правильном расположении порядка циклов существенно сокращается время умножения
Эфективно использовать кэш, распараллеливание всего, что разумно распараллелить

Решение СЛАУ
Итерационный метод - прибилженный ответ
Метод Гаусса (N^3 - сложность), Крамера (N^4) - точное решение
Итерационные методы(N^2) --> большая скорость вычислений по сравнению с методами Гаусса и Крамера!

Наука, изучающая решение СЛАУ работает с N ~ 10^6 (размеры матриц коэффициентов)

В каждом методе свои функции MPI (сложение, умножение...?)

Как получать матрицу А и вектор b? 
Не делать вариант с модельной задачей с заданным решением (3 вариант сходится за 1 итерацию)

Сгенерировать рандомную (на интервале [-100, 100]) симметричную матрицу (верхний треугольник с главной диагональю), затем отображаем
Потом на главной диагонали прибавить 200
Вектор b -  случайный на интервале [-100, 100]

2 варианта программы(на стр.5 - описание)
во 2 варианте матрицу можно разделить не построчно, а по столбцам

к субботе составить программу без MPI

На нулевом процессе сгенерировать матрицу А и раздать остальным 
(одной операцией MPI_Scatter - если одинаковые части,
если части разные -> MPI_scatter(v))

Аналогично с вектором b(но раздать каждому процессу целиком)
(MPI_Bcast - широковещательная рассылка)

Single Programm Multiple Data

############(I) Матрица раскидана построчно, вектор b целый у кадого процесса#################
Умножение матрицы на вектор
A_part(0, 1) - матрица из двух строк, если каждому процессу отдаем по две строки
Каждый i-ый процесс: 
    1) умножает свою часть матрицы на вектор b (A_part(0, 1) * X0 = X1(0, 1))
    2) X1(0, 1) - b(-, -) (нужно понять, какие координаты процессу нужно взять от вектора b)
    3) t * X1(0, 1) = X1(0, 1)
    4)X0(-, -) - X1(0, 1) = X1(0, 1) (координаты у X0 те же самые, что и у b)
    5)Нужно отправить часть полученного вектора X в полный вектор
        (у каждого процесса должен получиться полный вектор X - 
         реализовать через пересылку из всех процессов в один (gather) или allgather - из всех процессов во все
         если части неравные, то gather(v)/allgather(v))
    allgather(X1)

|X(n) - X(n-1)| <  Epsilon = 10^-5 ---> Сложно считать модуль вектора
Вектор может быть очень длинным
A(N * N), N ~ 10^(6,7)
Нормировать разность |X(n) - X(n-1)| / |b| < Epsilon ---> Считать нормально

|AX(n) - b| / |b| < Epsilon
Как считать модуль вектора параллельно?
Разрезать по частям вектор и посчитать отдельно суммы квадратов -> у каждого процесса по одному double
С помощью MPI можно выполнять операции во время передачи (reduce(+) - все числа сложить в одном процессе,
 allreduce(+) - результат сложения у каждого процесса), навесить корень на сумму

Модуль вектора b считать один раз
Чтобы не считать корень, можно один раз возвести эпсилон в квадрат один раз(либо задать заранее)
Сделать три(5, 7) оценки на эпсилон в глубину
Предыдущие модули считать не надо, т.к. они были посчитаны не предыдущих итерациях(запомнить их!)


###########(II)Матрицу A можно делить по столбцам, b - построчно################
Как раздать матицу А процессам?
Симметричную матрицу передвая по строкам передадим типо по столбцам
Можно также передать с помощью MPI_Scatter
0: gen(A)
all: scatter(A)
0: gen(b)
all: scatter(b)
all: gen(X0(0, 1)

allreduce(+) ---> 0:(w) + 1:(w) + 2:(w)

Каждый процесс генерирует по два элемента вектора X0

MPI_COMM_WORLD - предопределенный коммуникатор, в который входят???
Есть коммуникаторы, а есть группы
Оба для выделения множеств процессов из бОльшего множества
При приеме можно поставить большее число, чем было отправлено
Информация о размере полученных данных, источнике, тэге хранится в структуре st(status)


mpicxx test04_send_recv.c -o test_mpicxx.out
mpiicpc test04_send_recv.c -o test_mpiicpc.out
which mpicxx
which mpiicpc
mpicxx --version
mpiicpc --version

mpiexec -n 5 ./test_mpicxx.out
mpirun -n 5 ./test_mpicxx.out --> породится 5 процессов (если без mpirun, то породится только один процесс)

Относительная скорость работы процессов не определена
Никогда не можем полагаться на то, что один процесс завершится быстрее другого
send, recieve

N = 1000, 100 - размерность матрицы
Число процессов < сумма ядер на узлах
В отладочных целях использовать больше процессов

72 ядра, 380 гб ОЗУ


Построить графики ускорения и эффективности
Получили ли увеличение производительности после распараллеливания вычислений?

Критерии эффективности распараллеливания - ускорение, эффективность

Ускорение - Sp
Sp = T1/Tp (T1 - последовательная программа!, Tp - параллельная программа на p процессорных ядрах)
p = 2, 4, 8, 16, 24 - по 12 ядер на 2 машинах
Последовательная программа нагружает одно ядро
Вызов MPI функций жрет время!

Эффективность - Ep
Ep = Sp/p (эффективность варьируется в пределах от нуля до единицы)
0 < Ep <= 1


T1/T2 <= 2 (меньше двух из-за накладных расходов: передача по сети, вызов функций)
T1/(2*T2) <= 1 (когда достигает единицы - ИДЕАЛЬНОЕ УСКОРЕНИЕ)

На практике бывают ситуации, когда Ep > 1
L3 cache = 4Mb
Самый тяжелый объект - матрица А (весом например 6Mb -> она залезает в ОЗУ -- долго!)
Если матрица влезает в кэш - то нет постоянного обращения к ОЗУ

lim((n+7)/n) as n -> inf = 1

Одновременно сущесвтует порядка 100 процессов на 4 ядрах
Система переключается между ними
Процессы работают на ядре последовательно дрг за другом (псевдопараллельное выполнение)

Все процессы равноправные (с одинаковым приоритетом)
Можно ли на 24 ядрах запустить mpirun -n 30?
Можно, НО 

Процесс - контейнер для потоков
Работающей сущностью в процессе является поток
Процессы и потоки порождает операционная система

MPI_Bcast - рассылка данных от одного всем 
MPI_Gather - сборка из всех процессов по одной части в один кусок в одном процессе
MPI_Scatter - раздача данных из одного процесса по всем 
MPI_Allgather - сборка из всех по одной части во все процессы
(в каждом процессе получится одинаковый полный кусок)

Барьер - штука до которой дошли, подождали всех и пошли дальше
Как только все пришли в аудиторию(отметились, т.е. поставили отметку где-то;
тот, кто отметился, может сидеть заниматься своими делами), дальше можно начинать лекцию

Асинхронный барьер - место в программе, до которого все дошли и пошли дальше
Процесс может выполнять свои дела, после того как отметился и ждет

Подобрать параметры, чтобы программа работала хотя бы 30 секунд на последовательной программе!!!
(N = 2048)

E(24) = 90%(не требуется)
Если эффективность меньше 50%, то надо задуматься

T(24) = 30 секунд
T(1) = S(24)*T(24) = 0.9 * 24 * 30 = 648 секунд - долго ждать

E(p) = S(p)/p; S(p) = T(1)/T(p)


Графики: S, E, T (1, 2, 4, 8 ,16, 24 - минимальное число точек на графике(6 шт)) (с шагом 2 лучше)

На кластере несколько очередей
Есть узлы, нацеленные на вычисления на ЦПУ(без видеокарт),
также имеются узлы, нацеленные на вычисления на видеокартах

Можно поставить задачу в конкретную очередь!
(модифицировать файл bash)
Задачи обычно кидаются в short очередь
Для попадания в middle очередь придется подождать

При достижении 800 секунд система скорее всего кинет в middle очередь


Для генерации кода для видеокарт нужны специальные компиляторы
nvcc - Nvidia компилятор


opencl - более ... компилятор от AMD

На ЦПУ - десятки ядер
На GPU - сотни/тысячи ядер 

Профилировщик на кластере:
traceanalyzer - графическая утилита для вывода графика с кластера на пресональном компьютере
На windows: Xming

local: ssh -X hpcuser231@clunusc.ru

Следить за количеством итераций: не более 100 000



MPI_2:
A and b
1)scatter
2)scatter

vector parallel mult
3)allreduce

//CYCLE
matrix mult vector
4)reduce
5)scatter

vectors parallel mult
6)allreduce
vectors parallel mult
7)allreduce
vectors parallel mult
8)allreduce
//END OF CYCLE

9)gather



MPI_1:
1)scatter
2)bcast 

//CYCLE
###matrix mult vector###
3)allgather
